#!/bin/bash
### BEGIN INIT INFO
# Provides:          spark
# Required-Start:    $local_fs $network rsyslog $all
# Required-Stop:
# Should-Start:
# Default-Start:     2 3 4 5
# Default-Stop:      0 1 6
# X-Interactive:     true
# Short-Description: One-time app setup on first boot.
### END INIT INFO
set -eu -o pipefail

do_config() {
    curl --retry 3 --silent --show-error --fail --output /etc/user-data http://169.254.169.254/2008-02-01/user-data
    source /etc/user-data

    MEM_KB=`cat /proc/meminfo | grep MemTotal | awk '{print $2}'`
    #leave 25% of the available memory for the OS for cache buffers. If OS has too little memory large spark shuffle writes degrade in performace and eventually fail. sournce: http://spark.incubator.apache.org/docs/latest/hardware-provisioning.html
    MEM=$[(MEM_KB * 75 / 100) / 1024]
    MEMG=$[MEM / 1024]
    [ $MEM -lt 1024 ] && MEM=1024

    echo "export SCALA_HOME='/opt/scala'" > /opt/spark/conf/spark-env.sh

    devices=( $(find /dev -maxdepth 1 -regex '.*xvd[bcde]' -printf "%p,") )
    tmp_dirs=${devices//\/dev\//\/tmp\/spark-}
    tmp_dirs=${tmp_dirs%,}

    if [ $MEMG -le 32 ]
    then
        echo "export SPARK_JAVA_OPTS='-Dspark.local.dir=${tmp_dirs} -Dspark.akka.frameSize=500 -Dspark.akka.askTimeout=60 -Dspark.worker.timeout=600 -Dspark.akka.timeout=200 -Dspark.shuffle.consolidateFiles=true -Dspark.rdd.compress=true -XX:+UseCompressedOops -XX:+UseParallelGC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/dump'" >> /opt/spark/conf/spark-env.sh
    else
        echo "export SPARK_JAVA_OPTS='-Dspark.local.dir=${tmp_dirs} -Dspark.akka.frameSize=500 -Dspark.akka.askTimeout=60 -Dspark.worker.timeout=600 -Dspark.akka.timeout=200 -Dspark.shuffle.consolidateFiles=true -Dspark.rdd.compress=true                        -XX:+UseParallelGC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/dump'" >> /opt/spark/conf/spark-env.sh
    fi

    # Needed or jobs will fail with Too many files open
    echo "ulimit -n 524288" >> /opt/spark/conf/spark-env.sh

    # Needed for large shuffle reads so it does not cause OutOfMemoryException on MapInputTracker
    echo "sysctl -w vm.max_map_count=524240" >> /opt/spark/conf/spark-env.sh

    # Master and slave daemons pick up SPARK_DAEMON_JAVA_OPTS
    echo "export SPARK_DAEMON_JAVA_OPTS='-Dspark.worker.timeout=600 -Dspark.akka.frameSize=500 -Dspark.akka.askTimeout=60 -Dspark.akka.timeout=200 -Dspark.shuffle.consolidateFiles=true -Dspark.rdd.compress=true -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/dump'" >> /opt/spark/conf/spark-env.sh

    if [ "$SERVER_ROLE" = 'master' ]
    then
        MASTER_IP="master.${CLUSTER_NAME}.${DNS_DOMAIN}"
        echo "export SPARK_MASTER_IP=$MASTER_IP" >> /opt/spark/conf/spark-env.sh
        echo "export SPARK_EXECUTOR_MEMORY=${MEM}m" >> /opt/spark/conf/spark-env.sh
    else
        PUBLIC_DNS=$(wget -q -O - http://instance-data.ec2.internal/latest/meta-data/public-hostname)
        echo "export SPARK_PUBLIC_DNS=$PUBLIC_DNS" >> /opt/spark/conf/spark-env.sh
        #maximum memory per job (can run multiple jobs at the same time)
        echo "export SPARK_EXECUTOR_MEMORY=${MEM}m" >> /opt/spark/conf/spark-env.sh
        #maximum memory per spark worker
        echo "export SPARK_WORKER_MEMORY=${MEM}m" >> /opt/spark/conf/spark-env.sh
        echo "export SPARK_WORKER_DIR=/tmp/spark-xvdb/spark-work" >> /opt/spark/conf/spark-env.sh
        #SPARK_MASTER_PORT
        #SPARK_MASTER_WEBUI_PORT, to use non-default ports
        #SPARK_WORKER_CORES, to set the number of cores to use on this machine
        #SPARK_WORKER_PORT
        #SPARK_WORKER_WEBUI_PORT
    fi

    chmod 755 /opt/spark/conf/spark-env.sh
}

do_start() {
    echo "Starting spark"
    source /etc/user-data

    if [ "$SERVER_ROLE" = 'master' ]
    then
        /opt/spark/sbin/start-master.sh
    else
        # Need to resolve hostname to ip, otherwise we get some akka shittiness.
        MASTER_IP="master.${CLUSTER_NAME}.${DNS_DOMAIN}"
        /opt/spark/sbin/start-slave.sh 1 spark://$MASTER_IP:7077
    fi
}

do_stop() {
    echo "Stopping Spark"
    source /etc/user-data
    kill -9 $(pgrep -f "java.*spark") > /dev/null 2>&1 || true
}

case $1 in
    start)
        do_config
        do_start
        ;;
    stop)
        do_stop
        ;;
    restart)
        do_stop
        do_start
        ;;
    *)
        echo "Usage: $0 {start|stop|restart}"
        ;;
esac
