#!/bin/bash
### BEGIN INIT INFO
# Provides:          spark
# Required-Start:    $local_fs $network rsyslog $all
# Required-Stop:
# Should-Start:
# Default-Start:     2 3 4 5
# Default-Stop:      0 1 6
# X-Interactive:     true
# Short-Description: One-time app setup on first boot.
### END INIT INFO
set -eu -o pipefail

do_config() {
    curl --retry 3 --silent --show-error --fail --output /etc/user-data http://169.254.169.254/2008-02-01/user-data
    source /etc/user-data

    MEM_KB=`cat /proc/meminfo | grep MemTotal | awk '{print $2}'`
    #leave 25% of the available memory for the OS for cache buffers. If OS has too little memory large spark shuffle writes degrade in performace and eventually fail. sournce: http://spark.incubator.apache.org/docs/latest/hardware-provisioning.html
    MEM=$[(MEM_KB * 75 / 100) / 1024]
    MEMG=$[MEM / 1024]

    if [ $MEM -lt 1024 ]
    then
        MEM=1024
    fi

    echo "export SCALA_HOME='/opt/scala'" > /opt/spark/conf/spark-env.sh

    devices=( $(find /dev -maxdepth 1 -regex '.*xvd[bcde]' -printf "%p,") )
    tmp_dirs=${devices//\/dev\//\/tmp\/spark-}
    tmp_dirs=${tmp_dirs%,}

    if [ $MEMG -le 32 ]
    then
        echo "export SPARK_JAVA_OPTS='-Dspark.local.dir=${tmp_dirs} -Dspark.akka.frameSize=500 -Dspark.akka.askTimeout=60 -Dspark.worker.timeout=600 -Dspark.akka.timeout=200 -Dspark.shuffle.consolidateFiles=true -Dspark.rdd.compress=true -Dspark.akka.threads=8 -Dspark.task.maxFailures=12 -XX:+UseCompressedOops -XX:+UseParallelGC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/dump'" >> /opt/spark/conf/spark-env.sh
    else
        echo "export SPARK_JAVA_OPTS='-Dspark.local.dir=${tmp_dirs} -Dspark.akka.frameSize=500 -Dspark.akka.askTimeout=60 -Dspark.worker.timeout=600 -Dspark.akka.timeout=200 -Dspark.shuffle.consolidateFiles=true -Dspark.rdd.compress=true -Dspark.akka.threads=8 -Dspark.task.maxFailures=12                        -XX:+UseParallelGC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/dump'" >> /opt/spark/conf/spark-env.sh
    fi
    
    # Needed or jobs will fail with Too many files open
    echo "ulimit -n 524288" >> /opt/spark/conf/spark-env.sh

    # Needed for large shuffle reads so it does not cause OutOfMemoryException on MapInputTracker 
    echo "sysctl -w vm.max_map_count=524240" >> /opt/spark/conf/spark-env.sh
    
    # Master and slave daemons pick up SPARK_DAEMON_JAVA_OPTS
    echo "export SPARK_DAEMON_JAVA_OPTS='-Dspark.worker.timeout=600 -Dspark.akka.frameSize=500 -Dspark.akka.askTimeout=60 -Dspark.akka.timeout=200 -Dspark.shuffle.consolidateFiles=true -Dspark.rdd.compress=true -Dspark.akka.threads=8 -Dspark.task.maxFailures=12 -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/dump'" >> /opt/spark/conf/spark-env.sh
    
    if [ "$SERVER_ROLE" = 'master' ]
    then
        MASTER_IP=$(wget -q -O - http://instance-data.ec2.internal/latest/meta-data/public-hostname)
        echo "export SPARK_MASTER_IP=$MASTER_IP" >> /opt/spark/conf/spark-env.sh
        echo "export SPARK_MEM=${MEM}m" >> /opt/spark/conf/spark-env.sh

        curl --silent -X PUT -H 'Content-Type:' --data-binary "{\"Status\" : \"SUCCESS\",\"Reason\" : \"Configuration Complete\",\"UniqueId\" : \"ID1234\",\"Data\" : \"${MASTER_IP}\"}"  "$SIGNAL_URL"
    else
        PUBLIC_DNS=$(wget -q -O - http://instance-data.ec2.internal/latest/meta-data/public-hostname)
        echo "export SPARK_PUBLIC_DNS=$PUBLIC_DNS" >> /opt/spark/conf/spark-env.sh
	#maximum memory per job (can run multiple jobs at the same time)
        echo "export SPARK_MEM=${MEM}m" >> /opt/spark/conf/spark-env.sh
	#maximum memory per spark worker 
	echo "export SPARK_WORKER_MEMORY=${MEM}m" >> /opt/spark/conf/spark-env.sh
        echo "export SPARK_WORKER_DIR=/tmp/spark-xvdb/spark-work" >> /opt/spark/conf/spark-env.sh
        #SPARK_MASTER_PORT
        #SPARK_MASTER_WEBUI_PORT, to use non-default ports
        #SPARK_WORKER_CORES, to set the number of cores to use on this machine
        #SPARK_WORKER_MEMORY, to set how much memory to use (e.g. 1000m, 2g)
        #SPARK_WORKER_PORT
        #SPARK_WORKER_WEBUI_PORT
    fi

    chmod 755 /opt/spark/conf/spark-env.sh
}

do_start() {
    echo "Starting spark"
    source /etc/user-data

    if [ "$SERVER_ROLE" = 'master' ]
    then
        /opt/spark/bin/start-master.sh
    else
        MASTER_IP=$(echo $MASTER_IP | sed -ne "s/.*:\(.*\)\}/\1/p")
        /opt/spark/bin/start-slave.sh 1 spark://$MASTER_IP:7077
    fi
}

do_stop() {
    echo "Stopping Spark"
    source /etc/user-data
    kill -9 $(pgrep -f "java.*spark") > /dev/null 2>&1 || true
}

case $1 in
    start)
        do_config
        do_start
        ;;
    stop)
        do_stop
        ;;
    restart)
        do_stop
        do_start
        ;;
    *)
        echo "Usage: $0 {start|stop|restart}"
        ;;
esac
